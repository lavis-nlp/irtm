#
#          IRTM TEXT MAPPER CONFIGURATION
#    -----------------------------------------
#
#        see the default.yml for comments
#

text_encoder: bert-base-cased

wandb_args:
  project: irtm-text
  log_model: false

trainer_args:
  gpus: 1
  replace_sampler_ddp: false

  # this is overwritten if needed
  fast_dev_run: false
  # accumulation logic is controlled
  # by the batch size
  accumulate_grad_batches: 1
  # --------------------
  # max_epochs: 2
  # limit_train_batches: 1
  # limit_val_batches: 1

  check_val_every_n_epoch: 2
  log_every_n_steps: 5

# clipping logic is implemented in irtm
# (see mapper forward pass)
clip_val: 1

checkpoint_args:
  mode: max
  monitor: inductive/both.realistic.hits_at_10
  save_top_k: 10

early_stopping_args:
  mode: max
  monitor: inductive/both.realistic.hits_at_10

dataloader_train_args:
  num_workers: 0
  shuffle: true

dataloader_valid_args:
  num_workers: 0

dataloader_test_args:
  num_workers: 0

