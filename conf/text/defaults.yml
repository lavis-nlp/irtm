#
#          IRTM TEXT MAPPER CONFIGURATION
#    -----------------------------------------
#
# - use this file as a template to copy your own configuration
# - you can provide multiple config files which are merged (see common/ryaml)
# - this is nice if you want to change gpus: simply mixin your batch sizes


dataset: null
kgc_model: null


#
#  TRAINING
#

# weights and biases configuration
# see https://docs.wandb.com/integrations/lightning#wandblogger
# the following attributes are set by irtm:
#   - name
#   - save_dir
# the following attributes are set by irtm but can be overwritten:
#   - offline
wandb_args:
  project: null
  log_model: False

# https://pytorch-lightning.readthedocs.io/en/stable/trainer.html#trainer-class-api
# https://github.com/PyTorchLightning/pytorch-lightning/blob/9acee67c31c84dac74cc6169561a483d3b9c9f9d/pytorch_lightning/trainer/trainer.py#L81
trainer_args:
  gpus: null
  max_epochs: null
  fast_dev_run: null
  log_every_n_steps: null
  replace_sampler_ddp: null
  # only change this if you know why you do it
  accumulate_grad_batches: 1

  # gradient_clip_val must not be set!
  # see: https://github.com/PyTorchLightning/pytorch-lightning/issues/7698
  # use the global clip_val option instead
  # gradient_clip_val: null

clip_val: null


# checkpointing
# see https://pytorch-lightning.readthedocs.io/en/latest/generated/pytorch_lightning.callbacks.ModelCheckpoint.html?highlight=ModelCheckpoint
# and https://pykeen.readthedocs.io/en/stable/api/pykeen.evaluation.RankBasedMetricResults.html
#
# possible metrics:n
#    general syntax:
#    <KIND>/<PYKEEN-IDENTIFIER>
#
#    e.g:
#      inductive/both.realistic.hits_at_10
#
checkpoint_args:
  monitor: null
  save_top_k: null


# NOTE: untested for multi gpu!
# whether to use a sampler
# supported sampler types:
#   node degree: sample weighted by node degree
#     (https://pytorch.org/docs/stable/data.html#torch.utils.data.WeightedRandomSampler)
# if this is provided, set the trainer arg replace_sampler_ddp: False
sampler: null
sampler_args:
  # either an integer, x<INT> (a multiplier), or 'triples'
  # e.g.: x3 means #samples * 3
  num_samples: null
  replacement: null

# IRT dataset path
dataset: null

# pytorch dataloader
# https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader
dataloader_train_args:
  batch_size: null
  num_workers: null
  shuffle: True

dataloader_valid_args:
  batch_size: null
  num_workers: null

dataloader_test_args:
  batch_size: null
  num_workers: null

optimizer: null
optimizer_args:
  lr: null

# learning rate scheduling
# supported schedulers are:
# https://huggingface.co/transformers/main_classes/optimizer_schedules.html
#    - constant
#    - constant with warmup
#    - cosine with warmup
#    - cosine with hard restarts with warmup
#    - linear with warmup
scheduler: null
scheduler_args:
  num_warmup_steps: null
  # ...

# early stopping
# supported arguments:
# https://pytorch-lightning.readthedocs.io/en/latest/early_stopping.html
# https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.callbacks.early_stopping.html#pytorch_lightning.callbacks.early_stopping.EarlyStopping
early_stopping: null
early_stopping_args:
  monitor: null
  min_delta: null
  patience: null
  mode: null
  # ...

#
#  MAPPER
#

# huggingface transformer implementation
text_encoder: null
# whether to fine-tune the transformer
freeze_text_encoder: null

# look inside irtm/text/mapper.py to find module names and see the
# respective <Class>.impl dictionary for available implementations and
# possibly <Class>.Config for the necessary configuration
aggregator: null
reductor: null
projector: null
comparator: null

aggregator_args: null
reductor_args: null
projector_args: null
comparator_args: null

out: null
# directory to write to:
#   defaults to:
#     irtm.ENV.TEXT_DIR /
#     mapper /
#     <text_dataset> /   # e.g. oke.fb15k237_30061990_50
#     <text_database> /  # e.g. contexts-v7-enwiki-20200920-100-500.db
#     <text_model> /     # e.g. bert-base-cased
#     <kgc_model>        # e.g. distmult
# there is always a timestamp appended to the path
# you can reference these variables in your own format:
# out: my/own/directory/{split_dataset}/{kgc_model}/
